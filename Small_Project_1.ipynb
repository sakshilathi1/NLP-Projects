{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vN01WYgxQBn"
      },
      "outputs": [],
      "source": [
        "# Clean installs with compatible pins to avoid resolver conflicts\n",
        "!pip -q install -U pip setuptools wheel\n",
        "\n",
        "# Core libs for the project\n",
        "!pip -q install \"datasets>=2.20\" \"transformers>=4.43\" \"accelerate>=0.33\" \\\n",
        "                \"trl>=0.9.6\" \"peft>=0.12\" evaluate pandas numpy tqdm regex\n",
        "\n",
        "# Quantization for QLoRA\n",
        "!pip -q install bitsandbytes\n",
        "\n",
        "# Professor extras\n",
        "!pip -q install wandb llama-recipes\n",
        "\n",
        "# --- Compatibility pins to satisfy preinstalled packages in Colab ---\n",
        "# google-adk -> needs PyYAML >=6.0.2,<7\n",
        "# Flask/Werkzeug want MarkupSafe >=2.1.1\n",
        "# pymc wants rich >=13.7.1\n",
        "!pip -q install \"pyyaml>=6.0.2,<7\" \"markupsafe>=2.1.5\" \"rich>=13.7.1\"\n",
        "\n",
        "# Quick sanity print so we can see resolved versions\n",
        "import pkgutil, importlib\n",
        "for mod in [\"yaml\", \"markupsafe\", \"rich\"]:\n",
        "    m = importlib.import_module(mod)\n",
        "    print(mod, getattr(m, \"__version__\", \"ok\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean out unrelated preinstalled packages that pin conflicting versions.\n",
        "# These are not needed for this assignment.\n",
        "!pip -q uninstall -y llama-cookbook semgrep bigframes pymc google-adk || true\n",
        "\n",
        "# Install versions that work great with our stack (and are widely compatible)\n",
        "!pip -q install \"pyyaml>=6.0.2,<7\" \"markupsafe>=2.1.5\" \"rich==13.7.1\"\n",
        "\n",
        "# Sanity check\n",
        "import importlib\n",
        "for mod in [\"yaml\", \"markupsafe\", \"rich\"]:\n",
        "    m = importlib.import_module(mod)\n",
        "    print(mod, getattr(m, \"__version__\", \"ok\"))\n"
      ],
      "metadata": {
        "id": "kiS4DYxB0R3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [MY ADDITION] Only needed if the model is gated and asks for a token\n",
        "# from huggingface_hub import login\n",
        "# login(\"paste you HF token here\")\n"
      ],
      "metadata": {
        "id": "jj30Qq--xSF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [MY ADDITION] Common imports + dirs\n",
        "import os, re, json, time, random, gc\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "from tqdm.auto import tqdm\n",
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    default_data_collator, DataCollatorForSeq2Seq\n",
        ")\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# Try MemoryTrace; define a no-op fallback if not available\n",
        "try:\n",
        "    from llama_recipes.utils.memory_utils import MemoryTrace\n",
        "except Exception:\n",
        "    class MemoryTrace:\n",
        "        def __enter__(self): return self\n",
        "        def __exit__(self, *args): pass\n",
        "        def print_stats(self):\n",
        "            try:\n",
        "                mem_alloc = torch.cuda.max_memory_allocated()/(1024**3)\n",
        "                mem_reserved = torch.cuda.max_memory_reserved()/(1024**3)\n",
        "                print(f\"Max CUDA memory allocated was {mem_alloc:.1f} GB\")\n",
        "                print(f\"Max CUDA memory reserved was {mem_reserved:.1f} GB\")\n",
        "            except: pass\n",
        "\n",
        "# Dirs\n",
        "OUT_DIR  = \"outputs\";     os.makedirs(OUT_DIR, exist_ok=True)\n",
        "DATA_DIR = \"data\";        os.makedirs(DATA_DIR, exist_ok=True)\n",
        "CKPT_DIR = \"checkpoints\"; os.makedirs(CKPT_DIR, exist_ok=True)\n",
        "\n",
        "# Repro\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "\n",
        "# Minor speedup\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "print(\"CUDA?\", torch.cuda.is_available())\n"
      ],
      "metadata": {
        "id": "sYMJOuhgxSWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 3.9 — BitsAndBytes fix & sanity checks ===\n",
        "!pip -q install -U bitsandbytes accelerate\n",
        "\n",
        "import torch, importlib, os\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "try:\n",
        "    bnb = importlib.import_module(\"bitsandbytes\")\n",
        "    print(\"bitsandbytes version:\", getattr(bnb, \"__version__\", \"unknown\"))\n",
        "except Exception as e:\n",
        "    print(\"bitsandbytes import failed:\", e)\n",
        "\n",
        "# Helps with CUDA memory fragmentation when (re)loading models\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n"
      ],
      "metadata": {
        "id": "J64th9uIVJqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 4 (UPDATED) — Load Llama-3.2-3B-Instruct with QLoRA if available ===\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from transformers.utils import is_bitsandbytes_available\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import torch, gc, os\n",
        "\n",
        "MODEL_ID = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "\n",
        "gc.collect(); torch.cuda.empty_cache()\n",
        "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "use_4bit = is_bitsandbytes_available(check_library_only=True) and use_cuda\n",
        "\n",
        "print(f\"CUDA available: {use_cuda}\")\n",
        "print(f\"bitsandbytes available: {is_bitsandbytes_available(check_library_only=True)}\")\n",
        "print(\"Attempting\", \"4-bit QLoRA load\" if use_4bit else \"fallback (no 4-bit)\")\n",
        "\n",
        "if use_4bit:\n",
        "    bnb_cfg = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16 if use_cuda else torch.float32,\n",
        "    )\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        quantization_config=bnb_cfg,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "else:\n",
        "    # Fallback: non-quantized (heavier). Keep bfloat16/float16 on GPU if possible.\n",
        "    dtype = torch.bfloat16 if use_cuda else torch.float32\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        torch_dtype=dtype,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Prepare for training\n",
        "model.config.use_cache = False\n",
        "\n",
        "if use_4bit:\n",
        "    # QLoRA path\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "    model.gradient_checkpointing_enable()\n",
        "\n",
        "    lora_cfg = LoraConfig(\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        r=16, lora_alpha=32, lora_dropout=0.05,\n",
        "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
        "    )\n",
        "    model = get_peft_model(model, lora_cfg)\n",
        "    print(\"[QLoRA] Trainable parameters:\")\n",
        "    model.print_trainable_parameters()\n",
        "else:\n",
        "    # If we couldn’t load 4-bit, we *can still* attach LoRA, but VRAM will be tighter.\n",
        "    try:\n",
        "        model.gradient_checkpointing_enable()\n",
        "        lora_cfg = LoraConfig(\n",
        "            task_type=\"CAUSAL_LM\",\n",
        "            r=16, lora_alpha=32, lora_dropout=0.05,\n",
        "            target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
        "        )\n",
        "        model = get_peft_model(model, lora_cfg)\n",
        "        print(\"[Fallback] LoRA attached without 4-bit. Consider reducing context_length to 320–384.\")\n",
        "        model.print_trainable_parameters()\n",
        "    except Exception as e:\n",
        "        print(\"[Fallback] Could not attach LoRA:\", e)\n",
        "        print(\"You can still run Zero-Shot baseline. For SFT on Colab T4, 4-bit is strongly recommended.\")\n"
      ],
      "metadata": {
        "id": "L2w1uf6LxSbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [EDIT FROM PROF] Simple check the pipeline runs\n",
        "prompt = \"A robe takes 2 bolts of blue fiber and half that much white fiber. How many bolts in total does it take?\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "with torch.no_grad():\n",
        "    out = model.generate(**inputs, max_new_tokens=40, do_sample=False, temperature=0.0)\n",
        "text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "print(text)\n",
        "print(\"Model reply:\", text[len(prompt):])\n"
      ],
      "metadata": {
        "id": "1k8CmD-AxfP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [MY ADDITION / FIX] Handles HF dict-of-lists batches + allows live model OR path (full model / PEFT adapters)\n",
        "\n",
        "import os, re, json, torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from tqdm.auto import tqdm as tq\n",
        "\n",
        "# Ensure outputs dir exists\n",
        "try:\n",
        "    OUT_DIR\n",
        "except NameError:\n",
        "    OUT_DIR = \"outputs\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# CoT prompt (put this in your report)\n",
        "COT_PROMPT = \"\"\"{question}\n",
        "\n",
        "Solve the problem step by step. Keep steps concise.\n",
        "End with the exact final answer on a new last line as: \"#### <number>\"\n",
        "\n",
        "Let's think step by step.\n",
        "\"\"\"\n",
        "\n",
        "NUM_RE = re.compile(r\"####\\s*(-?\\d+(?:\\.\\d+)?)\")\n",
        "\n",
        "def extract_final(text: str):\n",
        "    if not text:\n",
        "        return None\n",
        "    m = NUM_RE.search(text)\n",
        "    return m.group(1).strip() if m else None\n",
        "\n",
        "def batched_generate(model, tokenizer, prompts, max_new_tokens=128, temperature=0.0, device=\"cuda\"):\n",
        "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False if temperature == 0.0 else True,\n",
        "            temperature=temperature,\n",
        "            use_cache=True,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            return_dict_in_generate=False,\n",
        "        )\n",
        "    texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "    gens = []\n",
        "    for p, t in zip(prompts, texts):\n",
        "        i = t.rfind(p)\n",
        "        gens.append(t[i+len(p):].strip() if i != -1 else t.strip())\n",
        "    return gens\n",
        "\n",
        "def load_model_and_tokenizer_for_eval(model_or_path, default_base=None):\n",
        "    if default_base is None:\n",
        "        default_base = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "        if \"MODEL_ID\" in globals() and isinstance(MODEL_ID, str):\n",
        "            default_base = MODEL_ID\n",
        "\n",
        "    if not isinstance(model_or_path, str):\n",
        "        tok = globals().get(\"tokenizer\", None)\n",
        "        if tok is None:\n",
        "            tok = AutoTokenizer.from_pretrained(default_base, use_fast=True)\n",
        "            if tok.pad_token_id is None:\n",
        "                tok.pad_token_id = tok.eos_token_id\n",
        "        return model_or_path, tok\n",
        "\n",
        "    # Try PEFT adapter folder first\n",
        "    try:\n",
        "        from peft import AutoPeftModelForCausalLM\n",
        "        mdl = AutoPeftModelForCausalLM.from_pretrained(\n",
        "            model_or_path,\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
        "        )\n",
        "        base_name = None\n",
        "        try:\n",
        "            peft_cfg = mdl.peft_config\n",
        "            if isinstance(peft_cfg, dict) and len(peft_cfg) > 0:\n",
        "                for v in peft_cfg.values():\n",
        "                    if hasattr(v, \"base_model_name_or_path\") and v.base_model_name_or_path:\n",
        "                        base_name = v.base_model_name_or_path\n",
        "                        break\n",
        "        except:\n",
        "            pass\n",
        "        if base_name is None:\n",
        "            base_name = default_base\n",
        "        tok = AutoTokenizer.from_pretrained(base_name, use_fast=True)\n",
        "        if tok.pad_token_id is None:\n",
        "            tok.pad_token_id = tok.eos_token_id\n",
        "        return mdl, tok\n",
        "    except Exception:\n",
        "        # Fallback to full model dir\n",
        "        tok = AutoTokenizer.from_pretrained(model_or_path, use_fast=True)\n",
        "        if tok.pad_token_id is None:\n",
        "            tok.pad_token_id = tok.eos_token_id\n",
        "        mdl = AutoModelForCausalLM.from_pretrained(\n",
        "            model_or_path,\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
        "        )\n",
        "        return mdl, tok\n",
        "\n",
        "def eval_em(model_or_path, split=\"test\", outpath=f\"{OUT_DIR}/eval.jsonl\",\n",
        "            batch_size=6, temperature=0.0, max_new=128, limit=None):\n",
        "    ds_full = load_dataset(\"openai/gsm8k\", \"main\")[split]\n",
        "    N = len(ds_full) if limit is None else min(limit, len(ds_full))\n",
        "\n",
        "    mdl, tok = load_model_and_tokenizer_for_eval(model_or_path)\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    em = 0; recs = []\n",
        "    for start in tq(range(0, N, batch_size), desc=f\"Eval {split} ({N} ex)\"):\n",
        "        end = min(start + batch_size, N)\n",
        "        b = ds_full[start:end]  # dict-of-lists\n",
        "        if isinstance(b, dict):\n",
        "            questions = b[\"question\"]; answers = b[\"answer\"]\n",
        "        else:\n",
        "            subset = ds_full.select(range(start, end))\n",
        "            questions = subset[\"question\"]; answers = subset[\"answer\"]\n",
        "\n",
        "        prompts = [COT_PROMPT.format(question=q) for q in questions]\n",
        "        gens = batched_generate(mdl, tok, prompts, max_new_tokens=max_new, temperature=temperature, device=device)\n",
        "\n",
        "        for q, gold_text, g in zip(questions, answers, gens):\n",
        "            m = NUM_RE.search(gold_text)\n",
        "            gold = m.group(1) if m else None\n",
        "            pred = extract_final(g)\n",
        "            ok = int(pred is not None and gold is not None and pred == gold)\n",
        "            em += ok\n",
        "            recs.append({\"q\": q, \"gold\": gold, \"pred\": pred, \"gen\": g, \"em\": ok})\n",
        "\n",
        "    with open(outpath, \"w\") as f:\n",
        "        for r in recs: f.write(json.dumps(r) + \"\\n\")\n",
        "\n",
        "    acc = em / N if N > 0 else 0.0\n",
        "    print(f\"EM: {acc:.4f}  ({em}/{N})  | batch_size={batch_size}, max_new={max_new}, temp={temperature}\")\n",
        "    return acc\n"
      ],
      "metadata": {
        "id": "yyOl0SLOxhbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [MY ADDITION] Baseline EM — quick 200-sample run\n",
        "baseline_em_200 = eval_em(\n",
        "    model,\n",
        "    split=\"test\",\n",
        "    outpath=f\"{OUT_DIR}/baseline_test200.jsonl\",\n",
        "    batch_size=6,\n",
        "    temperature=0.0,\n",
        "    max_new=128,\n",
        "    limit=200\n",
        ")\n",
        "baseline_em_200\n",
        "\n",
        "# Full test later (longer): remove limit\n",
        "# baseline_em = eval_em(model, split=\"test\",\n",
        "#                       outpath=f\"{OUT_DIR}/baseline_test.jsonl\",\n",
        "#                       batch_size=6, temperature=0.0, max_new=128)\n",
        "# baseline_em\n"
      ],
      "metadata": {
        "id": "COSIKCg6xlqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [PROFESSOR CODE] cleaned to use padding path (simpler & stable on Colab)\n",
        "\n",
        "def save_to_json(output_filename, train_step_loss, train_epoch_loss, train_step_ppl, train_epoch_ppl, val_step_loss, val_epoch_loss, val_step_ppl, val_epoch_ppl):\n",
        "    metrics_data = {\n",
        "        \"train_step_loss\": train_step_loss,\n",
        "        \"train_epoch_loss\": train_epoch_loss,\n",
        "        \"train_step_perplexity\": train_step_ppl,\n",
        "        \"train_epoch_perplexity\": train_epoch_ppl,\n",
        "        \"val_step_loss\": val_step_loss,\n",
        "        \"val_epoch_loss\": val_epoch_loss,\n",
        "        \"val_step_perplexity\": val_step_ppl,\n",
        "        \"val_epoch_perplexity\": val_epoch_ppl\n",
        "    }\n",
        "    with open(output_filename, \"w\") as f: json.dump(metrics_data, f)\n",
        "\n",
        "def get_dataloader_kwargs(train_config, tokenizer, mode):\n",
        "    kwargs = {}\n",
        "    batch_size = train_config.batch_size_training if mode==\"train\" else train_config.val_batch_size\n",
        "    kwargs[\"batch_size\"] = batch_size\n",
        "    kwargs[\"drop_last\"] = True\n",
        "    kwargs[\"collate_fn\"] = DataCollatorForSeq2Seq(tokenizer)\n",
        "    return kwargs\n",
        "\n",
        "import contextlib\n",
        "@contextlib.contextmanager\n",
        "def profile(cfg, local_rank=None):\n",
        "    if cfg.use_profiler:\n",
        "        wait_step, warmup_step, active_step = 1, 2, 3\n",
        "        print(f\"Profiler active; saving to {cfg.profiler_dir}\")\n",
        "        with torch.profiler.profile(\n",
        "            activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n",
        "            schedule=torch.profiler.schedule(wait=wait_step, warmup=warmup_step, active=active_step, repeat=1),\n",
        "            on_trace_ready=torch.profiler.tensorboard_trace_handler(cfg.profiler_dir),\n",
        "            profile_memory=True, with_stack=False, with_flops=True, record_shapes=True,\n",
        "        ) as torch_profiler:\n",
        "            yield torch_profiler\n",
        "    else:\n",
        "        yield None\n",
        "\n",
        "def evaluation(model, train_config, eval_dataloader, tokenizer, wandb_run=None):\n",
        "    model.eval()\n",
        "    val_step_loss, val_step_perplexity = [], []\n",
        "    eval_loss = 0.0\n",
        "    with MemoryTrace() as memtrace:\n",
        "        for step, batch in enumerate(tqdm(eval_dataloader, colour=\"green\", desc=\"evaluating\", dynamic_ncols=True)):\n",
        "            for k in batch: batch[k] = batch[k].to('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**batch); loss = outputs.loss\n",
        "                val_step_loss.append(loss.detach().float().item())\n",
        "                val_step_perplexity.append(float(torch.exp(loss.detach().float())))\n",
        "                eval_loss += loss.detach().float()\n",
        "    eval_epoch_loss = eval_loss / max(1, len(eval_dataloader))\n",
        "    eval_ppl = torch.exp(eval_epoch_loss)\n",
        "    return eval_ppl, eval_epoch_loss, val_step_loss, val_step_perplexity\n",
        "\n",
        "def train(model, train_dataloader, eval_dataloader, tokenizer, optimizer, lr_scheduler, gradient_accumulation_steps, train_config, wandb_run=None):\n",
        "    autocast = contextlib.nullcontext\n",
        "    train_prep, train_loss, val_prep, val_loss = [], [], [], []\n",
        "    if train_config.save_metrics:\n",
        "        metrics_filename = f\"{train_config.output_dir}/metrics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "        train_step_perplexity, train_step_loss = [], []\n",
        "        val_step_loss, val_step_perplexity = [], []\n",
        "\n",
        "    epoch_times, checkpoint_times = [], []\n",
        "    best_val_loss = float(\"inf\")\n",
        "    total_train_steps = 0\n",
        "    max_steps_reached = False\n",
        "\n",
        "    for epoch in range(train_config.num_epochs):\n",
        "        if max_steps_reached: break\n",
        "        epoch_start = time.perf_counter()\n",
        "        with MemoryTrace() as memtrace:\n",
        "            model.train()\n",
        "            total_loss = 0.0\n",
        "            total_length = len(train_dataloader)//gradient_accumulation_steps\n",
        "            pbar = tqdm(colour=\"blue\", desc=f\"Training Epoch: {epoch+1}\", total=total_length, dynamic_ncols=True)\n",
        "            with profile(train_config, None) as prof:\n",
        "                for step, batch in enumerate(train_dataloader):\n",
        "                    total_train_steps += 1\n",
        "                    if train_config.max_train_step > 0 and total_train_steps > train_config.max_train_step:\n",
        "                        max_steps_reached = True; print(\"Reached max_train_step; stopping.\"); break\n",
        "                    for k in batch: batch[k] = batch[k].to('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "                    with autocast():\n",
        "                        outputs = model(**batch); loss = outputs.loss\n",
        "                    loss = loss / gradient_accumulation_steps\n",
        "\n",
        "                    if train_config.save_metrics:\n",
        "                        train_step_loss.append(loss.detach().float().item())\n",
        "                        train_step_perplexity.append(float(torch.exp(loss.detach().float())))\n",
        "\n",
        "                    total_loss += loss.detach().float()\n",
        "                    loss.backward()\n",
        "\n",
        "                    if (step + 1) % gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n",
        "                        if train_config.gradient_clipping and train_config.gradient_clipping_threshold > 0.0:\n",
        "                            torch.nn.utils.clip_grad_norm_(model.parameters(), train_config.gradient_clipping_threshold)\n",
        "                        optimizer.step(); optimizer.zero_grad(); pbar.update(1)\n",
        "\n",
        "                    if train_config.use_profiler and prof is not None: prof.step()\n",
        "                    pbar.set_description(f\"Epoch {epoch+1}/{train_config.num_epochs} step {step}/{len(train_dataloader)} (loss {float(loss):.4f})\")\n",
        "                pbar.close()\n",
        "\n",
        "        epoch_time = time.perf_counter()-epoch_start\n",
        "        epoch_times.append(epoch_time)\n",
        "        train_epoch_loss = total_loss / max(1, len(train_dataloader))\n",
        "        train_perplexity = torch.exp(train_epoch_loss)\n",
        "        train_prep.append(float(train_perplexity)); train_loss.append(float(train_epoch_loss))\n",
        "        memtrace.print_stats()\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        if train_config.run_validation and eval_dataloader is not None:\n",
        "            eval_ppl, eval_epoch_loss, temp_val_loss, temp_step_perplexity = evaluation(model, train_config, eval_dataloader, tokenizer, wandb_run)\n",
        "            val_loss.extend([float(eval_epoch_loss)])\n",
        "            val_prep.extend([float(eval_ppl)])\n",
        "            ckpt_start = time.perf_counter()\n",
        "            if train_config.save_model:\n",
        "                epoch_dir = os.path.join(train_config.output_dir, f\"epoch{epoch}\")\n",
        "                os.makedirs(epoch_dir, exist_ok=True)\n",
        "                model.save_pretrained(epoch_dir)\n",
        "                print(f\"Model saved to {epoch_dir}\")\n",
        "            checkpoint_times.append(time.perf_counter()-ckpt_start)\n",
        "            if eval_epoch_loss < best_val_loss:\n",
        "                best_val_loss = eval_epoch_loss; print(f\"best eval loss @ epoch {epoch+1}: {best_val_loss:.4f}\")\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: train_ppl={train_perplexity:.4f}, train_loss={train_epoch_loss:.4f}, time={epoch_time:.1f}s\")\n",
        "        if train_config.save_metrics:\n",
        "            save_to_json(metrics_filename, train_step_loss, train_loss, train_step_perplexity, train_prep, val_step_loss, val_loss, val_step_perplexity, val_prep)\n",
        "\n",
        "    results = {\n",
        "        'avg_train_prep': np.mean(train_prep) if train_prep else None,\n",
        "        'avg_train_loss': np.mean(train_loss) if train_loss else None,\n",
        "        'avg_eval_prep':  np.mean(val_prep)   if val_prep   else None,\n",
        "        'avg_eval_loss':  np.mean(val_loss)   if val_loss   else None,\n",
        "        'avg_epoch_time': np.mean(epoch_times) if epoch_times else None,\n",
        "        'avg_checkpoint_time': np.mean(checkpoint_times) if checkpoint_times else None,\n",
        "    }\n",
        "    if train_config.save_metrics: results[\"metrics_filename\"] = metrics_filename\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "5SLjH2AOxnPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [EDIT FROM PROF] Train on gold reasoning from GSM8K train\n",
        "def get_preprocessed_dataset(tokenizer, max_train=0):\n",
        "    ds_all = load_dataset(\"openai/gsm8k\", \"main\")\n",
        "    train = ds_all[\"train\"]\n",
        "    if max_train and max_train > 0:\n",
        "        train = train.select(range(max_train))\n",
        "\n",
        "    # 10% val split\n",
        "    train_ds, val_ds = train.train_test_split(test_size=0.1, seed=SEED).values()\n",
        "\n",
        "    def tok(sample):\n",
        "        prompt_ids = tokenizer.encode(tokenizer.bos_token + \"###Input:\\n\" + sample[\"question\"] + \"\\n\", add_special_tokens=False)\n",
        "        label_ids  = tokenizer.encode(\"###Output:\\n\" + sample[\"answer\"] + tokenizer.eos_token, add_special_tokens=False)\n",
        "        ids = prompt_ids + label_ids\n",
        "        return {\"input_ids\": ids, \"attention_mask\": [1]*len(ids), \"labels\": ids}\n",
        "\n",
        "    train_ds = train_ds.map(tok, remove_columns=list(train_ds.features))\n",
        "    val_ds   = val_ds.map(tok,   remove_columns=list(val_ds.features))\n",
        "    return train_ds, val_ds\n"
      ],
      "metadata": {
        "id": "FqA45-e6xqCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [EDIT FROM PROF] Colab-safe defaults; \"padding\" batching; QLoRA optimizer\n",
        "\n",
        "@dataclass\n",
        "class train_configy:\n",
        "    model_name: str = MODEL_ID\n",
        "    tokenizer_name: str = None\n",
        "    run_validation: bool = False          # FAST: no val during train (enable for full runs)\n",
        "    batch_size_training: int = 1\n",
        "    batching_strategy: str = \"padding\"    # simpler & stable on Colab\n",
        "    context_length: int = 384             # shorter seq helps speed/memory\n",
        "    gradient_accumulation_steps: int = 2  # effective batch ~2\n",
        "    gradient_clipping: bool = True\n",
        "    gradient_clipping_threshold: float = 1.0\n",
        "    num_epochs: int = 1\n",
        "    max_train_step: int = 200             # FAST: cap steps; set 0 for full\n",
        "    max_eval_step: int = 0\n",
        "    num_workers_dataloader: int = 0\n",
        "    lr: float = 2e-4                      # LoRA-friendly LR\n",
        "    weight_decay: float = 0.0\n",
        "    gamma: float = 0.85\n",
        "    seed: int = SEED\n",
        "    mixed_precision: bool = True\n",
        "    val_batch_size: int = 1\n",
        "    output_dir: str = \"./content/vanilla_sft\"\n",
        "    save_model: bool = True\n",
        "    save_metrics: bool = False\n",
        "    flop_counter: bool = False\n",
        "    flop_counter_start: int = 3\n",
        "    use_profiler: bool = False\n",
        "    profiler_dir: str = \"./content/vanilla_sft\"\n",
        "\n",
        "train_config = train_configy()\n",
        "\n",
        "dataset_train, dataset_val = get_preprocessed_dataset(tokenizer)\n",
        "print(f\"Train size: {len(dataset_train)} | Val size: {len(dataset_val)}\")\n",
        "\n",
        "dl_kwargs_train = get_dataloader_kwargs(train_config, tokenizer, \"train\")\n",
        "dl_kwargs_val   = get_dataloader_kwargs(train_config, tokenizer, \"val\")\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    dataset_train, num_workers=train_config.num_workers_dataloader, pin_memory=True, **dl_kwargs_train\n",
        ")\n",
        "eval_dataloader = torch.utils.data.DataLoader(\n",
        "    dataset_val,   num_workers=train_config.num_workers_dataloader, pin_memory=True, **dl_kwargs_val\n",
        ")\n",
        "\n",
        "# Optimizer on LoRA params only\n",
        "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = optim.AdamW(trainable_params, lr=train_config.lr, weight_decay=train_config.weight_decay)\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=train_config.gamma)\n",
        "\n",
        "print(\"Loaders ready. Trainable params:\", sum(p.numel() for p in trainable_params))\n"
      ],
      "metadata": {
        "id": "PSnoguHOxr0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_vanilla = train(\n",
        "    model,\n",
        "    train_dataloader,\n",
        "    eval_dataloader if train_config.run_validation else None,\n",
        "    tokenizer,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    train_config.gradient_accumulation_steps,\n",
        "    train_config,\n",
        "    None,\n",
        ")\n",
        "[print(f'Key: {k}, Value: {v}') for k, v in results_vanilla.items()];\n",
        "\n",
        "# Save adapters (optional; included for submission)\n",
        "os.makedirs(train_config.output_dir, exist_ok=True)\n",
        "model.save_pretrained(train_config.output_dir)\n",
        "\n",
        "# Evaluate Vanilla SFT — use live model (includes LoRA adapters)\n",
        "vanilla_em_200 = eval_em(\n",
        "    model,\n",
        "    split=\"test\",\n",
        "    outpath=f\"{OUT_DIR}/vanilla_test200.jsonl\",\n",
        "    batch_size=6,\n",
        "    temperature=0.0,\n",
        "    max_new=128,\n",
        "    limit=200\n",
        ")\n",
        "vanilla_em_200\n"
      ],
      "metadata": {
        "id": "1cINW0hUxtdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Recovery Prelude: (re)define dirs & tokenizer if missing ---\n",
        "import os, torch\n",
        "try:\n",
        "    OUT_DIR\n",
        "except NameError:\n",
        "    OUT_DIR = \"outputs\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "try:\n",
        "    DATA_DIR\n",
        "except NameError:\n",
        "    DATA_DIR = \"data\"\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "# Make sure MODEL_ID & tokenizer exist\n",
        "try:\n",
        "    MODEL_ID\n",
        "except NameError:\n",
        "    MODEL_ID = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "\n",
        "try:\n",
        "    tokenizer\n",
        "except NameError:\n",
        "    from transformers import AutoTokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
        "    if tokenizer.pad_token_id is None:\n",
        "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        " `"
      ],
      "metadata": {
        "id": "BbjDepEVjTy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 12 (UPDATED) — STaR bootstrapping (robust batching; uses dtype; no global in defaults) =====\n",
        "import os, json, torch\n",
        "from tqdm.auto import tqdm as tq\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Prompt used when the first rationale was wrong; include this in your report\n",
        "HINT_PROMPT = \"\"\"{question}\n",
        "\n",
        "You are given that the correct final answer is: {final_answer}\n",
        "Reverse-engineer a correct, logically-sound step-by-step solution that ends with:\n",
        "#### {final_answer}\n",
        "\"\"\"\n",
        "\n",
        "def build_star_data(model_or_path, out_path=None, temp=0.0, max_new=128, bsz=6, limit=None):\n",
        "    # choose default save path lazily so it never errors if DATA_DIR wasn't set earlier\n",
        "    if out_path is None:\n",
        "        out_path = os.path.join(DATA_DIR, \"star_train_iter1.jsonl\")\n",
        "\n",
        "    ds_tr_full = load_dataset(\"openai/gsm8k\",\"main\")[\"train\"]\n",
        "    N = len(ds_tr_full) if limit is None else min(limit, len(ds_tr_full))\n",
        "    ds = ds_tr_full.select(range(N)) if N < len(ds_tr_full) else ds_tr_full\n",
        "\n",
        "    # Accept a live model or an HF model id / local path\n",
        "    if isinstance(model_or_path, str):\n",
        "        tok = AutoTokenizer.from_pretrained(model_or_path, use_fast=True)\n",
        "        if tok.pad_token_id is None:\n",
        "            tok.pad_token_id = tok.eos_token_id\n",
        "        dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
        "        mdl = AutoModelForCausalLM.from_pretrained(\n",
        "            model_or_path,\n",
        "            dtype=dtype,              # <- use dtype (torch_dtype is deprecated)\n",
        "            device_map=\"auto\",\n",
        "        )\n",
        "    else:\n",
        "        mdl = model_or_path\n",
        "        tok = tokenizer\n",
        "\n",
        "    kept, rat = [], []\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    for start in tq(range(0, N, bsz), desc=\"STaR gen\"):\n",
        "        end = min(start + bsz, N)\n",
        "        batch = ds[start:end]  # HF returns dict-of-lists\n",
        "\n",
        "        if isinstance(batch, dict):\n",
        "            questions = batch[\"question\"]; answers = batch[\"answer\"]\n",
        "        else:\n",
        "            subset = ds.select(range(start, end))\n",
        "            questions = subset[\"question\"]; answers = subset[\"answer\"]\n",
        "\n",
        "        # First pass: regular CoT\n",
        "        prompts = [COT_PROMPT.format(question=q) for q in questions]\n",
        "        gens = batched_generate(mdl, tok, prompts, max_new_tokens=max_new,\n",
        "                                temperature=temp, device=device)\n",
        "\n",
        "        wrong_q, wrong_gold = [], []\n",
        "        for q, a, g in zip(questions, answers, gens):\n",
        "            m = NUM_RE.search(a)\n",
        "            gold = m.group(1) if m else None\n",
        "            pred = extract_final(g)\n",
        "            if gold is not None and pred == gold:\n",
        "                kept.append({\"question\": q, \"rationale\": g.strip(), \"final_answer\": gold})\n",
        "            else:\n",
        "                if gold is not None:\n",
        "                    wrong_q.append(q); wrong_gold.append(gold)\n",
        "\n",
        "        # Second pass: rationalize with the correct answer\n",
        "        if wrong_q:\n",
        "            rprompts = [HINT_PROMPT.format(question=q, final_answer=ga) for q, ga in zip(wrong_q, wrong_gold)]\n",
        "            rgens = batched_generate(mdl, tok, rprompts, max_new_tokens=max_new,\n",
        "                                     temperature=temp, device=device)\n",
        "            for q, ga, rg in zip(wrong_q, wrong_gold, rgens):\n",
        "                pred = extract_final(rg)\n",
        "                if pred == ga:\n",
        "                    rat.append({\"question\": q, \"rationale\": rg.strip(), \"final_answer\": ga})\n",
        "\n",
        "    rows = kept + rat\n",
        "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "    with open(out_path, \"w\") as f:\n",
        "        for r in rows:\n",
        "            f.write(json.dumps(r) + \"\\n\")\n",
        "    print(f\"Saved {len(rows)} rows ({len(kept)} correct-first, {len(rat)} rationalized) -> {out_path}\")\n",
        "    return {\"total\": len(rows), \"correct_first\": len(kept), \"rationalized\": len(rat)}\n",
        "\n",
        "# ---- Run STaR data build (FAST: subset; FULL: set limit=None) ----\n",
        "_ = build_star_data(\n",
        "    model_or_path=MODEL_ID,                     # same base you train\n",
        "    out_path=os.path.join(DATA_DIR, \"star_train_iter1.jsonl\"),\n",
        "    temp=0.0,\n",
        "    max_new=128,\n",
        "    bsz=6,\n",
        "    limit=1000                                  # change to None for full train set\n",
        ")\n"
      ],
      "metadata": {
        "id": "sY_WRzC3xvR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [MY ADDITION] Turn STaR JSONL into LM text and reuse QLoRA SFT loop\n",
        "\n",
        "def load_star_as_lm_text(jsonl_path):\n",
        "    rows=[]\n",
        "    with open(jsonl_path) as f:\n",
        "        for line in f:\n",
        "            ex = json.loads(line)\n",
        "            text = f\"{ex['question']}\\n\\n{ex['rationale']}\".strip()\n",
        "            rows.append({\"text\": text})\n",
        "    return Dataset.from_list(rows)\n",
        "\n",
        "star_ds = load_star_as_lm_text(f\"{DATA_DIR}/star_train_iter1.jsonl\")\n",
        "\n",
        "def tok_row(ex):\n",
        "    ids = tokenizer.encode(ex[\"text\"], add_special_tokens=False)\n",
        "    return {\"input_ids\": ids, \"attention_mask\": [1]*len(ids), \"labels\": ids}\n",
        "\n",
        "star_ds = star_ds.map(tok_row, remove_columns=list(star_ds.features))\n",
        "star_train, star_val = star_ds.train_test_split(test_size=0.05, seed=SEED).values()\n",
        "\n",
        "# New output dir\n",
        "star_out = \"./content/star_iter1\"; os.makedirs(star_out, exist_ok=True)\n",
        "\n",
        "# Reuse padding strategy\n",
        "dl_kwargs_train = get_dataloader_kwargs(train_config, tokenizer, \"train\")\n",
        "dl_kwargs_val   = get_dataloader_kwargs(train_config, tokenizer, \"val\")\n",
        "\n",
        "star_train_loader = torch.utils.data.DataLoader(star_train, num_workers=0, pin_memory=True, **dl_kwargs_train)\n",
        "star_val_loader   = torch.utils.data.DataLoader(star_val,   num_workers=0, pin_memory=True, **dl_kwargs_val)\n",
        "\n",
        "# Fresh optimizer/scheduler on LoRA params\n",
        "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = optim.AdamW(trainable_params, lr=train_config.lr, weight_decay=train_config.weight_decay)\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=train_config.gamma)\n",
        "\n",
        "# Use same FAST settings (you can later set max_train_step=0 for full)\n",
        "train_config.output_dir = star_out\n",
        "\n",
        "results_star1 = train(\n",
        "    model,\n",
        "    star_train_loader,\n",
        "    star_val_loader if train_config.run_validation else None,\n",
        "    tokenizer,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    train_config.gradient_accumulation_steps,\n",
        "    train_config,\n",
        "    None,\n",
        ")\n",
        "[print(f'Key: {k}, Value: {v}') for k, v in results_star1.items()];\n",
        "\n",
        "# Save adapters\n",
        "model.save_pretrained(star_out)\n",
        "\n",
        "# Evaluate (200-ex quick)\n",
        "star1_em_200 = eval_em(\n",
        "    model,  # live model with newest adapters\n",
        "    split=\"test\",\n",
        "    outpath=f\"{OUT_DIR}/star_iter1_test200.jsonl\",\n",
        "    batch_size=6,\n",
        "    temperature=0.0,\n",
        "    max_new=128,\n",
        "    limit=200\n",
        ")\n",
        "star1_em_200\n"
      ],
      "metadata": {
        "id": "l_8ijg0fx1eM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== Exact Match (GSM8K test) — Summary (fast subset) ===\")\n",
        "try:    print(f\"Zero-Shot CoT (200 ex): {baseline_em_200:.4f}\")\n",
        "except: print(\"Zero-Shot CoT (200 ex): not run yet\")\n",
        "\n",
        "try:    print(f\"Vanilla SFT (200 ex):  {vanilla_em_200:.4f}\")\n",
        "except: print(\"Vanilla SFT (200 ex):  not run yet\")\n",
        "\n",
        "try:    print(f\"STaR (iter 1, 200 ex): {star1_em_200:.4f}\")\n",
        "except: print(\"STaR (iter 1, 200 ex): not run yet\")\n"
      ],
      "metadata": {
        "id": "dTTPvrBZx5vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [MY ADDITION] Package code + outputs for submission\n",
        "!mkdir -p submission/code submission/report\n",
        "\n",
        "# Save prompts for the report\n",
        "with open(\"submission/report/prompts.txt\",\"w\") as f:\n",
        "    f.write(\"COT_PROMPT:\\n\" + COT_PROMPT + \"\\n\\n\")\n",
        "    f.write(\"HINT_PROMPT:\\n\" + HINT_PROMPT + \"\\n\")\n",
        "\n",
        "# Copy useful outputs\n",
        "!cp -r outputs submission/ 2>/dev/null || true\n",
        "!cp -r content submission/checkpoints 2>/dev/null || true\n",
        "\n",
        "# Zip it\n",
        "!zip -r submission.zip submission >/dev/null\n",
        "print(\"Created submission.zip\")\n"
      ],
      "metadata": {
        "id": "s1TBn7G0yCtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tEkE7Pa3xw6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZGkml6nwxSel"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}